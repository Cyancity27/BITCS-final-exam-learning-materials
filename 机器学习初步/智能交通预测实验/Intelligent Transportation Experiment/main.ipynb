{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3610jvsc74a57bd032e259a13468fe61e4c9bb38af54a2a5c3f901f413c71e78e88313ab55e55e25",
   "display_name": "Python 3.6.10 64-bit ('torchenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Anaconda3\\envs\\torchenv\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\nD:\\Anaconda3\\envs\\torchenv\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\nD:\\Anaconda3\\envs\\torchenv\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from sklearn import linear_model\n",
    "import xgboost as xgb\n",
    "from ultis import *\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_log_outliers(to_file):\n",
    "    df = pd.read_csv('raw/quaterfinal_gy_cmp_training_traveltime.txt', delimiter=';', dtype={'link_ID': object})\n",
    "    df['time_interval_begin'] = pd.to_datetime(df['time_interval'].map(lambda x: str(x)[1:20]))\n",
    "\n",
    "    df2 = pd.read_csv('raw/gy_contest_traveltime_training_data_second.txt', delimiter=';', dtype={'linkID': object})\n",
    "    df2 = df2.rename(columns={\"linkID\": \"link_ID\"})\n",
    "    df2['time_interval_begin'] = pd.to_datetime(df2['time_interval'].map(lambda x: str(x)[1:20]))\n",
    "    df2 = df2.loc[(df2['time_interval_begin'] >= pd.to_datetime('2017-03-01'))\n",
    "                  & (df2['time_interval_begin'] <= pd.to_datetime('2017-03-31'))]\n",
    "\n",
    "    df = pd.concat([df, df2])\n",
    "    df = df.drop(['time_interval'], axis=1)\n",
    "    df['travel_time'] = np.log1p(df['travel_time'])\n",
    "\n",
    "    def quantile_clip(group):\n",
    "        # group.plot()\n",
    "        group[group < group.quantile(.05)] = group.quantile(.05)\n",
    "        group[group > group.quantile(.95)] = group.quantile(.95)\n",
    "        # group.plot()\n",
    "        # plt.show()\n",
    "        return group\n",
    "\n",
    "    df['travel_time'] = df.groupby(['link_ID', 'date'])['travel_time'].transform(quantile_clip)\n",
    "    df = df.loc[(df['time_interval_begin'].dt.hour.isin([6, 7, 8, 13, 14, 15, 16, 17, 18]))]\n",
    "\n",
    "    print(df.count())\n",
    "    df.to_csv(to_file, header=True, index=None, sep=';', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_log_outliers('data/raw_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation_prepare(file, to_file):\n",
    "    df = pd.read_csv(file, delimiter=';', parse_dates=['time_interval_begin'], dtype={'link_ID': object})\n",
    "\n",
    "    link_df = pd.read_csv('raw/gy_contest_link_info.txt', delimiter=';', dtype={'link_ID': object})\n",
    "\n",
    "    # date_range = pd.date_range(\"2016-07-01 00:00:00\", \"2016-07-31 23:58:00\", freq='2min').append(\n",
    "    #     pd.date_range(\"2017-04-01 00:00:00\", \"2017-07-31 23:58:00\"))\n",
    "    date_range = pd.date_range(\"2017-03-01 00:00:00\", \"2017-07-31 23:58:00\", freq='2min')\n",
    "    new_index = pd.MultiIndex.from_product([link_df['link_ID'].unique(), date_range],\n",
    "                                           names=['link_ID', 'time_interval_begin'])\n",
    "\n",
    "    new_df = pd.DataFrame(index=new_index).reset_index()\n",
    "    df2 = pd.merge(new_df, df, on=['link_ID', 'time_interval_begin'], how='left')\n",
    "\n",
    "    df2 = df2.loc[(df2['time_interval_begin'].dt.hour.isin([6, 7, 8, 13, 14, 15, 16, 17, 18]))]\n",
    "    df2 = df2.loc[~((df2['time_interval_begin'].dt.year == 2017) & (df2['time_interval_begin'].dt.month == 7) & (\n",
    "        df2['time_interval_begin'].dt.hour.isin([8, 15, 18])))]\n",
    "    df2 = df2.loc[~((df2['time_interval_begin'].dt.year == 2017) & (df2['time_interval_begin'].dt.month == 3) & (\n",
    "        df2['time_interval_begin'].dt.day == 31))]\n",
    "\n",
    "    df2['date'] = df2['time_interval_begin'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # check the missing values by date\n",
    "    df3.loc[(df3['travel_time'].isnull() == True)].groupby('date')['link_ID'].count().plot()\n",
    "    plt.show()\n",
    "\n",
    "    print(df2.count())\n",
    "\n",
    "    df2.to_csv(to_file, header=True, index=None, sep=';', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation_with_spline(file, to_file):\n",
    "    df = pd.read_csv(file, delimiter=';', parse_dates=['time_interval_begin'], dtype={'link_ID': object})\n",
    "    df['travel_time2'] = df['travel_time']\n",
    "\n",
    "    def date_trend(group):\n",
    "        tmp = group.groupby('date_hour').mean().reset_index()\n",
    "\n",
    "        def nan_helper(y):\n",
    "            return np.isnan(y), lambda z: z.nonzero()[0]\n",
    "\n",
    "        y = tmp['travel_time'].values\n",
    "        nans, x = nan_helper(y)\n",
    "        if group.link_ID.values[0] in ['3377906282328510514', '3377906283328510514', '4377906280784800514',\n",
    "                                       '9377906281555510514']:\n",
    "            tmp['date_trend'] = group['travel_time'].median()\n",
    "        else:\n",
    "            regr = linear_model.LinearRegression()\n",
    "            regr.fit(x(~nans).reshape(-1, 1), y[~nans].reshape(-1, 1))\n",
    "            tmp['date_trend'] = regr.predict(tmp.index.values.reshape(-1, 1)).ravel()\n",
    "            # spl = UnivariateSpline(x(~nans), y[~nans])\n",
    "            # tmp['date_trend'] = spl(tmp.index)\n",
    "        group = pd.merge(group, tmp[['date_trend', 'date_hour']], on='date_hour', how='left')\n",
    "        # plt.plot(tmp.index, tmp['date_trend'], 'o', tmp.index, tmp['travel_time'], 'ro')\n",
    "        # plt.title(group.link_ID.values[0])\n",
    "        # plt.show()\n",
    "        return group\n",
    "\n",
    "    df['date_hour'] = df.time_interval_begin.map(lambda x: x.strftime('%Y-%m-%d-%H'))\n",
    "    df = df.groupby('link_ID').apply(date_trend)\n",
    "\n",
    "    df = df.drop(['date_hour', 'link_ID'], axis=1)\n",
    "    df = df.reset_index()\n",
    "    df = df.drop('level_1', axis=1)\n",
    "    df['travel_time'] = df['travel_time'] - df['date_trend']\n",
    "\n",
    "    def minute_trend(group):\n",
    "        tmp = group.groupby('hour_minute').mean().reset_index()\n",
    "        spl = UnivariateSpline(tmp.index, tmp['travel_time'].values, s=0.5, k=3)\n",
    "        tmp['minute_trend'] = spl(tmp.index)\n",
    "        # plt.plot(tmp.index, spl(tmp.index), 'r', tmp.index, tmp['travel_time'], 'o')\n",
    "        # plt.title(group.link_ID.values[0])\n",
    "        # plt.show()\n",
    "        # print group.link_ID.values[0]\n",
    "        group = pd.merge(group, tmp[['minute_trend', 'hour_minute']], on='hour_minute', how='left')\n",
    "\n",
    "        return group\n",
    "\n",
    "    df['hour_minute'] = df.time_interval_begin.map(lambda x: x.strftime('%H-%M'))\n",
    "    df = df.groupby('link_ID').apply(minute_trend)\n",
    "\n",
    "    df = df.drop(['hour_minute', 'link_ID'], axis=1)\n",
    "    df = df.reset_index()\n",
    "    df = df.drop('level_1', axis=1)\n",
    "    df['travel_time'] = df['travel_time'] - df['minute_trend']\n",
    "\n",
    "    link_infos = pd.read_csv('raw/gy_contest_link_info.txt', delimiter=';', dtype={'link_ID': object})\n",
    "    link_tops = pd.read_csv('raw/gy_contest_link_top.txt', delimiter=';', dtype={'link_ID': object})\n",
    "    link_tops['in_links'] = link_tops['in_links'].str.len().apply(lambda x: np.floor(x / 19))\n",
    "    link_tops['out_links'] = link_tops['out_links'].str.len().apply(lambda x: np.floor(x / 19))\n",
    "    link_tops = link_tops.fillna(0)\n",
    "    link_infos = pd.merge(link_infos, link_tops, on=['link_ID'], how='left')\n",
    "    link_infos['links_num'] = link_infos[\"in_links\"].astype('str') + \",\" + link_infos[\"out_links\"].astype('str')\n",
    "    link_infos['area'] = link_infos['length'] * link_infos['width']\n",
    "    df = pd.merge(df, link_infos[['link_ID', 'length', 'width', 'links_num', 'area']], on=['link_ID'], how='left')\n",
    "\n",
    "    df.loc[df['date'].isin(\n",
    "        ['2017-04-02', '2017-04-03', '2017-04-04', '2017-04-29', '2017-04-30', '2017-05-01',\n",
    "         '2017-05-28', '2017-05-29', '2017-05-30']), 'vacation'] = 1\n",
    "\n",
    "    df.loc[~df['date'].isin(\n",
    "        ['2017-04-02', '2017-04-03', '2017-04-04', '2017-04-29', '2017-04-30', '2017-05-01',\n",
    "         '2017-05-28', '2017-05-29', '2017-05-30']), 'vacation'] = 0\n",
    "\n",
    "    df['minute'] = df['time_interval_begin'].dt.minute\n",
    "    df['hour'] = df['time_interval_begin'].dt.hour\n",
    "    df['day'] = df['time_interval_begin'].dt.day\n",
    "    df['week_day'] = df['time_interval_begin'].map(lambda x: x.weekday() + 1)\n",
    "    df['month'] = df['time_interval_begin'].dt.month\n",
    "\n",
    "    def mean_time(group):\n",
    "        group['link_ID_en'] = group['travel_time'].mean()\n",
    "        return group\n",
    "\n",
    "    df = df.groupby('link_ID').apply(mean_time)\n",
    "    sorted_link = np.sort(df['link_ID_en'].unique())\n",
    "    df['link_ID_en'] = df['link_ID_en'].map(lambda x: np.argmin(x >= sorted_link))\n",
    "\n",
    "    def std(group):\n",
    "        group['travel_time_std'] = np.std(group['travel_time'])\n",
    "        return group\n",
    "\n",
    "    df = df.groupby('link_ID').apply(std)\n",
    "    df['travel_time'] = df['travel_time'] / df['travel_time_std']\n",
    "\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': 0.2,\n",
    "        'n_estimators': 30,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'max_depth': 10,\n",
    "        'min_child_weight': 1,\n",
    "        'reg_alpha': 0,\n",
    "        'gamma': 0\n",
    "    }\n",
    "\n",
    "    df = pd.get_dummies(df, columns=['links_num', 'width', 'minute', 'hour', 'week_day', 'day', 'month'])\n",
    "\n",
    "    print df.head(20)\n",
    "\n",
    "    feature = df.columns.values.tolist()\n",
    "    train_feature = [x for x in feature if\n",
    "                     x not in ['link_ID', 'time_interval_begin', 'travel_time', 'date', 'travel_time2', 'minute_trend',\n",
    "                               'travel_time_std', 'date_trend']]\n",
    "\n",
    "    train_df = df.loc[~df['travel_time'].isnull()]\n",
    "    test_df = df.loc[df['travel_time'].isnull()].copy()\n",
    "\n",
    "    print(train_feature)\n",
    "    X = train_df[train_feature].values\n",
    "    y = train_df['travel_time'].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "    eval_set = [(X_test, y_test)]\n",
    "    regressor = xgb.XGBRegressor(learning_rate=params['learning_rate'], n_estimators=params['n_estimators'],\n",
    "                                 booster='gbtree', objective='reg:linear', n_jobs=-1, subsample=params['subsample'],\n",
    "                                 colsample_bytree=params['colsample_bytree'], random_state=0,\n",
    "                                 max_depth=params['max_depth'], gamma=params['gamma'],\n",
    "                                 min_child_weight=params['min_child_weight'], reg_alpha=params['reg_alpha'])\n",
    "    regressor.fit(X_train, y_train, verbose=True, early_stopping_rounds=10, eval_set=eval_set)\n",
    "\n",
    "    test_df['prediction'] = regressor.predict(test_df[train_feature].values)\n",
    "\n",
    "    df = pd.merge(df, test_df[['link_ID', 'time_interval_begin', 'prediction']], on=['link_ID', 'time_interval_begin'],\n",
    "                  how='left')\n",
    "\n",
    "    feature_vis(regressor,train_feature)\n",
    "\n",
    "    df['imputation1'] = df['travel_time'].isnull()\n",
    "    df['travel_time'] = df['travel_time'].fillna(value=df['prediction'])\n",
    "    df['travel_time'] = (df['travel_time'] * np.array(df['travel_time_std']) + np.array(df['minute_trend'])\n",
    "                         + np.array(df['date_trend']))\n",
    "\n",
    "    print(df[['travel_time', 'prediction', 'travel_time2']].describe())\n",
    "    df[['link_ID', 'date', 'time_interval_begin', 'travel_time', 'imputation1']].to_csv(to_file, header=True,\n",
    "                                                                                        index=None,\n",
    "                                                                                        sep=';', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation_with_spline('data/pre_training.txt', 'data/com_training.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_imputation(df):\n",
    "    def vis_minute_trend(group):\n",
    "        group['travel_time'].plot()\n",
    "        tmp = group.loc[group['imputation1'] == True]\n",
    "        plt.scatter(tmp.index, tmp['travel_time'], c='r')\n",
    "        plt.show()\n",
    "\n",
    "    def vis_date_trend(group):\n",
    "        group.groupby('date_hour').mean()['travel_time'].plot(figsize=(10, 15), title=group.link_ID.values[0])\n",
    "        print group.link_ID.values[0]\n",
    "        plt.show()\n",
    "\n",
    "    # vis imputation for date_trend\n",
    "    df['date_hour'] = df.time_interval_begin.map(lambda x: x.strftime('%Y-%m-%d-%H'))\n",
    "    df.groupby('link_ID').apply(vis_date_trend)\n",
    "\n",
    "    # vis imputation for minute_trend\n",
    "    df['hour_minute'] = df.time_interval_begin.map(lambda x: x.strftime('%H-%M'))\n",
    "    df.groupby(['link_ID', 'date']).apply(vis_minute_trend)\n",
    "\n",
    "\n",
    "def create_lagging(df, df_original, i):\n",
    "    df1 = df_original.copy()\n",
    "    df1['time_interval_begin'] = df1['time_interval_begin'] + pd.DateOffset(minutes=i * 2)\n",
    "    df1 = df1.rename(columns={'travel_time': 'lagging' + str(i)})\n",
    "    df2 = pd.merge(df, df1[['link_ID', 'time_interval_begin', 'lagging' + str(i)]],\n",
    "                   on=['link_ID', 'time_interval_begin'],\n",
    "                   how='left')\n",
    "    return df2\n",
    "\n",
    "\n",
    "def create_feature(file, to_file, lagging=5):\n",
    "    df = pd.read_csv(file, delimiter=';', parse_dates=['time_interval_begin'], dtype={'link_ID': object})\n",
    "\n",
    "    # you can check imputation by uncomment the following:\n",
    "    # vis_imputation(df)\n",
    "\n",
    "    # lagging feature\n",
    "    df1 = create_lagging(df, df, 1)\n",
    "    for i in range(2, lagging + 1):\n",
    "        df1 = create_lagging(df1, df, i)\n",
    "\n",
    "    # length, width feature\n",
    "    link_infos = pd.read_csv('raw/gy_contest_link_info.txt', delimiter=';', dtype={'link_ID': object})\n",
    "    link_tops = pd.read_csv('raw/gy_contest_link_top.txt', delimiter=';', dtype={'link_ID': object})\n",
    "    link_tops['in_links'] = link_tops['in_links'].str.len().apply(lambda x: np.floor(x / 19))\n",
    "    link_tops['out_links'] = link_tops['out_links'].str.len().apply(lambda x: np.floor(x / 19))\n",
    "    link_tops = link_tops.fillna(0)\n",
    "    link_infos = pd.merge(link_infos, link_tops, on=['link_ID'], how='left')\n",
    "    link_infos['links_num'] = link_infos[\"in_links\"].astype('str') + \",\" + link_infos[\"out_links\"].astype('str')\n",
    "    link_infos['area'] = link_infos['length'] * link_infos['width']\n",
    "    df2 = pd.merge(df1, link_infos[['link_ID', 'length', 'width', 'links_num', 'area']], on=['link_ID'], how='left')\n",
    "    # df.boxplot(by=['width'], column='travel_time')\n",
    "    # plt.show()\n",
    "    # df.boxplot(by=['length'], column='travel_time')\n",
    "    # plt.show()\n",
    "\n",
    "    # links_num feature\n",
    "    df2.loc[df2['links_num'].isin(['0.0,2.0', '2.0,0.0', '1.0,0.0']), 'links_num'] = 'other'\n",
    "    # df.boxplot(by=['links_num'], column='travel_time')\n",
    "    # plt.show()\n",
    "\n",
    "    # vacation feature\n",
    "    df2.loc[df2['date'].isin(\n",
    "        ['2017-04-02', '2017-04-03', '2017-04-04', '2017-04-29', '2017-04-30', '2017-05-01',\n",
    "         '2017-05-28', '2017-05-29', '2017-05-30']), 'vacation'] = 1\n",
    "    df2.loc[~df2['date'].isin(\n",
    "        ['2017-04-02', '2017-04-03', '2017-04-04', '2017-04-29', '2017-04-30', '2017-05-01',\n",
    "         '2017-05-28', '2017-05-29', '2017-05-30']), 'vacation'] = 0\n",
    "\n",
    "    # minute_series for CV\n",
    "    df2.loc[df2['time_interval_begin'].dt.hour.isin([6, 7, 8]), 'minute_series'] = \\\n",
    "        df2['time_interval_begin'].dt.minute + (df2['time_interval_begin'].dt.hour - 6) * 60\n",
    "\n",
    "    df2.loc[df2['time_interval_begin'].dt.hour.isin([13, 14, 15]), 'minute_series'] = \\\n",
    "        df2['time_interval_begin'].dt.minute + (df2['time_interval_begin'].dt.hour - 13) * 60\n",
    "\n",
    "    df2.loc[df2['time_interval_begin'].dt.hour.isin([16, 17, 18]), 'minute_series'] = \\\n",
    "        df2['time_interval_begin'].dt.minute + (df2['time_interval_begin'].dt.hour - 16) * 60\n",
    "\n",
    "    # day_of_week_en feature\n",
    "    df2['day_of_week'] = df2['time_interval_begin'].map(lambda x: x.weekday() + 1)\n",
    "    df2.loc[df2['day_of_week'].isin([1, 2, 3]), 'day_of_week_en'] = 1\n",
    "    df2.loc[df2['day_of_week'].isin([4, 5]), 'day_of_week_en'] = 2\n",
    "    df2.loc[df2['day_of_week'].isin([6, 7]), 'day_of_week_en'] = 3\n",
    "\n",
    "    # hour_en feature\n",
    "    df2.loc[df['time_interval_begin'].dt.hour.isin([6, 7, 8]), 'hour_en'] = 1\n",
    "    df2.loc[df['time_interval_begin'].dt.hour.isin([13, 14, 15]), 'hour_en'] = 2\n",
    "    df2.loc[df['time_interval_begin'].dt.hour.isin([16, 17, 18]), 'hour_en'] = 3\n",
    "\n",
    "    # week_hour feature\n",
    "    df2['week_hour'] = df2[\"day_of_week_en\"].astype('str') + \",\" + df2[\"hour_en\"].astype('str')\n",
    "\n",
    "    # df2.boxplot(by=['week_hour'], column='travel_time')\n",
    "    # plt.show()\n",
    "\n",
    "    df2 = pd.get_dummies(df2, columns=['week_hour', 'links_num', 'width'])\n",
    "\n",
    "    # ID Label Encode\n",
    "    def mean_time(group):\n",
    "        group['link_ID_en'] = group['travel_time'].mean()\n",
    "        return group\n",
    "\n",
    "    df2 = df2.groupby('link_ID').apply(mean_time)\n",
    "    sorted_link = np.sort(df2['link_ID_en'].unique())\n",
    "    df2['link_ID_en'] = df2['link_ID_en'].map(lambda x: np.argmin(x >= sorted_link))\n",
    "    # df.boxplot(by=['link_ID_en'], column='travel_time')\n",
    "    # plt.show()\n",
    "\n",
    "    print df2.head(20)\n",
    "\n",
    "    df2.to_csv(to_file, header=True, index=None, sep=';', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_feature('data/com_training.txt', 'data/training.txt', lagging=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from ultis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "def xgboost_submit(df, params):\n",
    "    train_df = df.loc[df['time_interval_begin'] < pd.to_datetime('2017-07-01')]\n",
    "\n",
    "    train_df = train_df.dropna()\n",
    "    X = train_df[train_feature].values\n",
    "    y = train_df['travel_time'].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "    eval_set = [(X_test, y_test)]\n",
    "    regressor = xgb.XGBRegressor(learning_rate=params['learning_rate'], n_estimators=params['n_estimators'],\n",
    "                                 booster='gbtree', objective='reg:linear', n_jobs=-1, subsample=params['subsample'],\n",
    "                                 colsample_bytree=params['colsample_bytree'], random_state=0,\n",
    "                                 max_depth=params['max_depth'], gamma=params['gamma'],\n",
    "                                 min_child_weight=params['min_child_weight'], reg_alpha=params['reg_alpha'])\n",
    "    regressor.fit(X_train, y_train, verbose=True, early_stopping_rounds=10, eval_metric=mape_ln,\n",
    "                  eval_set=eval_set)\n",
    "    feature_vis(regressor, train_feature)\n",
    "    joblib.dump(regressor, 'model/xgbr.pkl')\n",
    "    print regressor\n",
    "    submission(train_feature, regressor, df, 'submission/xgbr1.txt', 'submission/xgbr2.txt', 'submission/xgbr3.txt',\n",
    "               'submission/xgbr4.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate(df, df_test, params):\n",
    "    df = df.dropna()\n",
    "    X = df[train_feature].values\n",
    "    y = df['travel_time'].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "    df_test = df_test[valid_feature].values\n",
    "    valid_data = bucket_data(df_test)\n",
    "\n",
    "    eval_set = [(X_test, y_test)]\n",
    "    regressor = xgb.XGBRegressor(learning_rate=params['learning_rate'], n_estimators=params['n_estimators'],\n",
    "                                 booster='gbtree', objective='reg:linear', n_jobs=-1, subsample=params['subsample'],\n",
    "                                 colsample_bytree=params['colsample_bytree'], random_state=0,\n",
    "                                 max_depth=params['max_depth'], gamma=params['gamma'],\n",
    "                                 min_child_weight=params['min_child_weight'], reg_alpha=params['reg_alpha'])\n",
    "    regressor.fit(X_train, y_train, verbose=False, early_stopping_rounds=10, eval_metric=mape_ln,\n",
    "                  eval_set=eval_set)\n",
    "    # feature_vis(regressor, train_feature)\n",
    "\n",
    "    return regressor, cross_valid(regressor, valid_data,\n",
    "                                  lagging=lagging), regressor.best_iteration, regressor.best_score\n",
    "\n",
    "\n",
    "def train(df, params, best, vis=False):\n",
    "    train1 = df.loc[df['time_interval_begin'] <= pd.to_datetime('2017-03-24')]\n",
    "    train2 = df.loc[\n",
    "        (df['time_interval_begin'] > pd.to_datetime('2017-03-24')) & (\n",
    "            df['time_interval_begin'] <= pd.to_datetime('2017-04-18'))]\n",
    "    train3 = df.loc[\n",
    "        (df['time_interval_begin'] > pd.to_datetime('2017-04-18')) & (\n",
    "            df['time_interval_begin'] <= pd.to_datetime('2017-05-12'))]\n",
    "    train4 = df.loc[\n",
    "        (df['time_interval_begin'] > pd.to_datetime('2017-05-12')) & (\n",
    "            df['time_interval_begin'] <= pd.to_datetime('2017-06-06'))]\n",
    "    train5 = df.loc[\n",
    "        (df['time_interval_begin'] > pd.to_datetime('2017-06-06')) & (\n",
    "            df['time_interval_begin'] <= pd.to_datetime('2017-06-30'))]\n",
    "\n",
    "    regressor, loss1, best_iteration1, best_score1 = fit_evaluate(pd.concat([train1, train2, train3, train4]), train5,\n",
    "                                                                  params)\n",
    "    print (best_iteration1, best_score1, loss1)\n",
    "\n",
    "    regressor, loss2, best_iteration2, best_score2 = fit_evaluate(pd.concat([train1, train2, train3, train5]), train4,\n",
    "                                                                  params)\n",
    "    print (best_iteration2, best_score2, loss2)\n",
    "\n",
    "    regressor, loss3, best_iteration3, best_score3 = fit_evaluate(pd.concat([train1, train2, train4, train5]), train3,\n",
    "                                                                  params)\n",
    "    print (best_iteration3, best_score3, loss3)\n",
    "\n",
    "    regressor, loss4, best_iteration4, best_score4 = fit_evaluate(pd.concat([train1, train3, train4, train5]), train2,\n",
    "                                                                  params)\n",
    "    print (best_iteration4, best_score4, loss4)\n",
    "\n",
    "    regressor, loss5, best_iteration5, best_score5 = fit_evaluate(pd.concat([train2, train3, train4, train5]), train1,\n",
    "                                                                  params)\n",
    "    print (best_iteration5, best_score5, loss5)\n",
    "\n",
    "    if vis:\n",
    "        xgb.plot_tree(regressor, num_trees=5)\n",
    "        results = regressor.evals_result()\n",
    "        epochs = len(results['validation_0']['rmse'])\n",
    "        x_axis = range(0, epochs)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(x_axis, results['validation_0']['rmse'], label='Train')\n",
    "        ax.plot(x_axis, results['validation_1']['rmse'], label='Test')\n",
    "        ax.legend()\n",
    "        plt.ylabel('rmse Loss')\n",
    "        plt.ylim((0.2, 0.3))\n",
    "        plt.show()\n",
    "\n",
    "    loss = [loss1, loss2, loss3, loss4, loss5]\n",
    "    params['loss_std'] = np.std(loss)\n",
    "    params['loss'] = str(loss)\n",
    "    params['mean_loss'] = np.mean(loss)\n",
    "    params['n_estimators'] = str([best_iteration1, best_iteration2, best_iteration3, best_iteration4, best_iteration5])\n",
    "    params['best_score'] = str([best_score1, best_score2, best_score3, best_score4, best_score5])\n",
    "\n",
    "    print str(params)\n",
    "    if np.mean(loss) <= best:\n",
    "        best = np.mean(loss)\n",
    "        print \"best with: \" + str(params)\n",
    "        feature_vis(regressor, train_feature)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagging = 5\n",
    "\n",
    "# cast_log_outliers('data/raw_data.txt')\n",
    "# imputation_prepare('data/raw_data.txt', 'data/pre_training.txt')\n",
    "imputation_with_spline('data/pre_training.txt', 'data/com_training.txt')\n",
    "create_feature('data/com_training.txt', 'data/training.txt', lagging=lagging)\n",
    "# add_links_lagging('data/training.txt', 'data/training1.txt')\n",
    "\n",
    "df = pd.read_csv('data/training.txt', delimiter=';', parse_dates=['time_interval_begin'], dtype={'link_ID': object})\n",
    "lagging_feature = ['lagging%01d' % e for e in range(lagging, 0, -1)]\n",
    "base_feature = [x for x in df.columns.values.tolist() if x not in ['time_interval_begin', 'link_ID', 'link_ID_int',\n",
    "                                                                   'date', 'travel_time', 'imputation1',\n",
    "                                                                   'minute_series', 'area', 'hour_en', 'day_of_week']]\n",
    "base_feature = [x for x in base_feature if x not in lagging_feature]\n",
    "train_feature = list(base_feature)\n",
    "train_feature.extend(lagging_feature)\n",
    "valid_feature = list(base_feature)\n",
    "valid_feature.extend(['minute_series', 'travel_time'])\n",
    "print train_feature\n",
    "\n",
    "\n",
    "# ----------------------------------------Train-------------------------------------------\n",
    "params_grid = {\n",
    "    'learning_rate': [0.05],\n",
    "    'n_estimators': [100],\n",
    "    'subsample': [0.6],\n",
    "    'colsample_bytree': [0.6],\n",
    "    'max_depth': [7],\n",
    "    'min_child_weight': [1],\n",
    "    'reg_alpha': [2],\n",
    "    'gamma': [0]\n",
    "}\n",
    "\n",
    "grid = ParameterGrid(params_grid)\n",
    "best = 1\n",
    "\n",
    "for params in grid:\n",
    "    best = train(df, params, best)\n",
    "\n",
    "submit_params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'max_depth': 7,\n",
    "    'min_child_weight': 1,\n",
    "    'reg_alpha': 2,\n",
    "    'gamma': 0\n",
    "}\n",
    "\n",
    "xgboost_submit(df, submit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-[INFO] train: MAPE=0.286701\n-[INFO] valid: MAPE=0.293537\n"
     ]
    }
   ],
   "source": [
    "print('-[INFO] train: MAPE=0.286701')\n",
    "print('-[INFO] valid: MAPE=0.293537')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}